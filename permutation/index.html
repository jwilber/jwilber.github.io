<!DOCTYPE html>
<html>
<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-84472984-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-84472984-1');
</script>
  <meta charset="utf-8">
  <title>Permutation Test R</title>
  <meta name="description" content="R Permutation Implementation">
  <meta name="author" content="Jared Wilber">

  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <meta content="IE=edge" http-equiv="X-UA-Compatible">
  <link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.5/paper/bootstrap.min.css" media="screen" rel="stylesheet">
  <link href="http://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
  <style>
  	img {
  		width:65%;
  		height: 500px;
  		margin: auto;
  	}
  </style>
</head>
<body>
	<div class="container">


<h1>Permutation Tests:</h1>
<h1> Explanation and Implementation</h1>
<p class="text-uppercase">by <a href="http://jwilber.github.io" target="_self">Jared Wilber</a> &middot; June 2016</p>

<p>The permutation test, also known as the randomization test, is a nonparametric method of statistical inference that tests a specific null hypothesis that the treatment levels we are comparing are completely equivalent and serve only as labels</p>

<p>Put more simply, when considering a permutation/randomization based analysis, our null hypothesis dictates that our response values are equally likely of ending up in any treatment group. Then, under that hypothesis, permuting our data will yield no effect on the outcome. </p>

<p>Our alternative hypothesis dictates that the data do come from different distributions; that there does exist some difference between treatments for one or more subjects.</p>

<p>To construct a permutation test, we select a test statistic for
the data, permute the data, and then calculate the test statistic for each permutation and build a test statistic distribution. </p>

<p>An expression for a two-tailed permutation test consisting of B permutations is shown below:</p>

<img src="/images/perm.png" />


<p>Unlike the more traditional statistical tests (e.g. t-tests or F-tests), the permutation test relies only on the performed randomization and, as such, we can ignore many annoying assumptions often required for linear models (such as normality, heteroscedastic residuals, etc.). That said, our data does need to meet the assumption of <a href="https://en.wikipedia.org/wiki/Exchangeable_random_variables">exchangeability</a> (i.e. can we permute the data without affecting the distribution?)</p>

<p>Carrying out a permutation test consists of 3 steps:</p>

<ul>
  <li> Come up with test statistic</li>
  <li>Permute Data and Obtain Sampling Distribution</li>
  <li>Conduct Permutation Test and Obtain P-Value</li>
  </ul>

<p>In what follows, I'll go into each step in detail and, along the way, reveal how one could implement each step in R.</p>

<hr>
<h2> Our Data and Brief EDA</h2>

<p>For ease of reproducibility, we'll create our own dataset. </p>

<p>We'll create a data set with two columns:</p>
	<ul>
  <li><b>SURVIVAL</b>: Survival in months after some operation was performed.</li>
  <li> <b>TREATMENT</b>: A boolean; A value of <code>TRUE</code> indicates that the unit received post-operation treatment. <code>FALSE</code> indicates that they did not (Control group).</li>
  </ul>
<p>For example, the response values in our dataset may refer to survival times after some breast cancer operation, where our <code>TREATMENT</code> refers to whether or not the individuals received some post-operation treatment or not.</p>

<p>The data is defined and showcased below:</p>

<div class="panel-body">
    <pre><code id="code-ex-1">
ourData <- data.frame(SURVIVAL = c(2,6,8,10,10,12,12,14,14,16,16,16,18,20,26,30,34,38,40,48,2,4,4,6,8,8,10,10,12,14,18,18,20,22,32,36,46,46,48,58,58,66,72,82),
TREATMENT = c(rep(FALSE,22),rep(TRUE,22)))
ourData
</code></pre>
  </div>

<center><img src="https://jwilber.github.io/permutation/images/dataset1.png" /></center>

<p>We'll use the following hypothesis scheme:</p>
	<ul>
  <li> <b>Null Hypothesis</b>: The means for the Control (FALSE) and Treatment (TRUE) groups are the same; i.e. the treatment has no effect on survival outcome.</li>
  <li> <b>Alternative Hypothesis</b>: The means for the Control (FALSE) and Treatment (TRUE) groups are different; i.e. the treatment does have some effect on survival times.</li>
	</ul>
<p>When performing any analysis, it's always a good idea to carry out some exploratory data analysis (EDA). For this example, we'll be brief and create a plot to visualize what we expect the outcome of our test will be:</p>
<br>
<center><img src="https://jwilber.github.io/permutation/images/box.png" width="450px" height="450px"/></center>
<br>

<p>The difference in centrality and spread certainly give credence to the hypothesis that the two groups come from different distributions.  Of course, we don't know for sure until we perform our test. Let's dive into our analysis and find out for sure.</p>

<hr>

<h2> 1. Come up with test statistic</h2>
<p>The first step of a permutation test is to come up with a relevant test-statistic. Recall that a test statistic is some numerical summary of our data that can be used during a statistical test to distinguish the null hypothesis from the alternative hypothesis. For a permutation test we can use essentially anything for our test statistic. The goal of our analysis is to detect any difference between the treatment and control groups. Therefore, we'll employ the difference of mean values between the two groups as our test statistic.</p> 

<div class="panel-body">
    <pre><code id="code-ex-1">
diffMeans <- function(data, hasTrt){
  # computes our test statistics: the difference of means
  # hasTrt: boolean vector, TRUE if has treatment
  test_stat <- mean(data[hasTrt]) - mean(data[!hasTrt])
  return(test_stat)
}
</code></pre>
  </div>


<code>currentStat <- diffMeans(ourData$SURVIVAL, ourData$TREATMENT)</code>
<br>
<code>cat("Initial Test Statistic: ", currentStat)</code>

<p>We calculate the test statistic for our initial data set, which we'll utilize later in our analysis.</p>

<div class="panel-body">
    <pre><code id="code-ex-1">
currentStat <- diffMeans(ourData$SURVIVAL, ourData$TREATMENT)
cat("Initial Test Statistic: ", currentStat)
</code></pre>
  </div>
<br>
<center><img src="https://jwilber.github.io/permutation/images/teststat1.png" /></center>
<br>
<hr>

<h2> 2. Permute Data and Obtain Sampling Distribution</h2>

<p>Once we've decided upon our test statistic, we permute our data. Permuting the data is just a fancy way of saying to rearrange or shuffle the data in all possible manners. As our null hypothesis states that shuffling the data should have no effect, this process creates the backbone of our permutation test: our randomization distribution. Recall the number of permutations for n elements is <code>factorial(n)</code>, a function grows faster than even <code>exponential(n)</code>. See below plot.</p>
<br>
<center><img src="https://jwilber.github.io/permutation/images/growth.png" width="450px" height="450px" /></center>
<br>


<p>Thus, even for moderately sized n, computing all permutations quickly becomes unfeasible. For example, <code>factorial(15)</code> is an astronomical 1,307,674,368,000. In comparison, <code>exp(15)</code> is a tiny 3,269,017.</p>

<p>To resolve this issue, instead of creating a distribution of all possible permutations, we'll simply build our permutation distribution by drawing some number, say k, of samples from the total permutation set. </p>

<p>We then calculate our test statistic for each generated sample. In this way, we build a distribution for our test statistic (our randomization distribution under the null).</p>

<p><i>Note</i> - this method will tend to be conservative for small sample sizes. For example, the smallest obtainable p-value (greater than 0) for 20 samples is 0.05. Hence, a sufficient sample size is necessary to attain an accurate p-value. Typically we'll use no less than 5,000 examples. The following implementation uses 10,000 samples.</p>



<div class="panel-body">
    <pre><code id="code-ex-1">
simPermDsn <- function(data, hasTrt, testStat, k=10000){
  # Simulates the permutation distribution for our data
  # hasTrt: boolean indicating whether group is treatment or control
  # testStat: function defining the test statistic to be computed
  # k: # of samples to build dsn.      
  sim_data   <- replicate(k, sample(data))
  test_stats <- apply(sim_data, 2,
                      function(x) {testStat(x, hasTrt)})
  return( test_stats)
}
</code></pre>
  </div>


<p>The above code simple creates k permutations of our data.</p>

<p>For our test statistic, we observe the following distribution of values:</p>
<br>
<center><img src="https://jwilber.github.io/permutation/images/hist1.png" width="450px" height="450px" /></center>
<br>

<p>I should also briefly clarify that while the two are often confused,the above sampling scheme is not the same as bootstrapping. Bootstrapping samples with replacement.</p>

<hr>

<h2>3. Conduct Permutation Test and Obtain P-Value</h2>


<p>Once we've created a sampling distribution for our test statistic, we can proceed to carry out the permutation test with ease. Acquiring our p-value is simple: we simply count the number of times we've obtained test statistics <i>as OR MORE EXTREME</i> than our initial test statistic, then divide that sum by the total number of samples.</p>

<p>Recall that our hypothesis was of the "two-way" form. Thus, in my code, I look for the sum of the <i>absolute value</i> of deviations greater than our initial test statistic, divided by the total number of permuted samples, k. For a one-way test, just sum those as-or-more-extreme observations in the relevant direction, then divide by k.</p>

<p>To perform our test, we'll encapsulate everything into one function as follows:</p>

<div class="panel-body">
    <pre><code id="code-ex-1">
permutationTest <- function(data, hasTrt, testStat, k=1000){
  # Performs permutation test for our data, given some pre-selected
  # test statistic, testStat
  currentStat    <- testStat(data, hasTrt)
  simulatedStats <- simPermDsn(data, hasTrt,testStat, k)
  
  # 2-way P-value
  pValue         <- sum(abs(simulatedStats) >= currentStat)  / k
  
  return(pValue)
}
</code></pre>
  </div>


<p>The first two lines of code in the above function define our initial test statistic and create our sampling distribution by calling previously defined functions. The third line calculates our p-value via the process stated above.</p>

<p>Congratulations! Now that we have defined our function, we've successfully implemented a permutation test!</p>

<p>So what are our results? Let's run our function on our data to find out:</p>

<div class="panel-body">
    <pre><code id="code-ex-1">
set.seed(619)
p.val <- permutationTest(ourData$SURVIVAL, ourData$TREATMENT, testStat = diffMeans)
cat("p-value: ", p.val)
</code></pre>
  </div>

<img src="/images/pvalues1.png" />

<p>So, our p-value is 0.03. </p>

<p>We can visualize this by observing our original test statistic's location in a sampled test statistic's distribution (symbolized by the vertical blue lines below). Recall that we calculated this earlier, obtaining a value of ~ 13:</p>
<br>
<center><img src="https://jwilber.github.io/permutation/images/hist2.png" width="450px" height="450px" /></center>
<br>
<p>This test statistic looks pretty deep in the tails of our distribution. In fact, with a significance level of 0.05, our p-value is significant. Thus we reject our null hypothesis and conclude that there is a difference provided by the treatment.</p>

<p>For sanity, we can check how our test agrees with the output of a parametric alternative; in this case, a two-sample t-test.</p>
 <hr>
<h2>Comparison with T-Test</h2>
 <p>We'll implement the test ourselves:</p>


<div class="panel-body">
    <pre><code id="code-ex-1">
tTest <- function(data, hasTrtment){
  m         <- sum(hasTrtment) # Treatment group size
  n         <- sum(!hasTrtment) # Control group size
  
  # difference of means (assuming unequal variances):
  mean_diff <- diffMeans(data=data, hasTrt=hasTrtment)
  # denominator:
  trt_var   <- var(data[hasTrtment])/m
  ctrl_var  <- var(data[!hasTrtment])/n
  sd_diff   <- sqrt(trt_var + ctrl_var)
  # t-statistic: 
  t_stat    <- mean_diff / sd_diff
  # p-value:
  df        <- min(n-1,m-1)
  pval      <- 1-pt(t_stat, df)
  
  return( pval )
}

tTest(ourData$SURVIVAL, ourData$TREATMENT)
</code></pre>
  </div>

<p>As expected, both tests yield significant results, thus leading to the same conclusion: we reject the null hypothesis and conclude that the treatment does have a significant effect on Survival time post-operation.</p>

<p>Note that, while it's nice to be able to implement your own permutation test, it's not always necessary. In particular, I know of two R packages: <code>coin</code> and <code>perm</code>, each of which allows us to conduct a permutation test wit a single line of code: </p>

<div class="panel-body">
    <pre><code id="code-ex-1">
# library(coin)
pvalc <- pvalue(independence_test(SURVIVAL~TREATMENT, data=ourData))

# library(perm)
pvalp <- permTS(SURVIVAL ~ TREATMENT, data=ourData, alternative = "two.sided", nmc=10000)$p.value

# compare p-values
cat(" p-value (from coin package) : ", round(pvalc,2), "\n",
    "p-value (from perm package) : ", round(pvalp,2), "\n",
    "p-value (our implementation): ", p.val)
</code></pre>
  </div>

<img src="/images/pvalues3.png" />

<p>Our implemented permutation test performed just as well as the packages' respective permutation tests. Thus, feel free to use whatever you prefer for your own analysis!</p>
<br>
<hr>

</div>


</body>
</html>